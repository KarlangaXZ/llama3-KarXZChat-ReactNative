# ğŸ¤– Chat App con Llama3 - React Native

Una aplicaciÃ³n de chat mÃ³vil que se conecta directamente con **Ollama** para conversar con **Llama3** de forma local.

## ğŸ“± Vista Previa

![ChatLlama](./exampleimg.jpg)


## âœ¨ CaracterÃ­sticas

- **ğŸ’¬ Chat en tiempo real** con Llama3
- **ğŸ¨ Interfaz moderna** estilo WhatsApp 
- **â° Timestamps** en todos los mensajes
- **ğŸ”„ Auto-scroll** a nuevos mensajes
- **âš¡ Loading states** y manejo de errores
- **ğŸ§¹ Limpiar historial** de chat
- **ğŸ“± Responsive** para iOS y Android

## ğŸš€ Inicio RÃ¡pido

### 1ï¸âƒ£ Instalar Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Descargar desde https://ollama.ai
```

### 2ï¸âƒ£ Ejecutar Llama3

```bash
ollama pull llama3
ollama serve
```

### 3ï¸âƒ£ Clonar y Ejecutar

```bash
git clone https://github.com/tu-usuario/chat-app-llama3.git
cd chat-app-llama3
npm install

# iOS
npx react-native run-ios

# Android  
npx react-native run-android
```

## ğŸ”§ ConfiguraciÃ³n

### Cambiar Modelo

En `App.tsx`, lÃ­nea 65:

```typescript
body: JSON.stringify({
  model: 'llama3',           // ğŸ‘ˆ Cambia aquÃ­
  messages: newMessages,
  stream: false,
}),
```

### Cambiar URL de Ollama

Si Ollama estÃ¡ en otro servidor:

```typescript
const response = await fetch('http://TU_IP:11434/api/chat', {
  // ...resto de configuraciÃ³n
});
```

## ğŸ“‹ Requisitos

- **Node.js** 18+
- **React Native CLI** o **Expo**
- **Ollama** ejecutÃ¡ndose en puerto 11434
- **iOS Simulator** / **Android Emulator**

## ğŸ› ï¸ TecnologÃ­as

- **React Native** - Framework mÃ³vil
- **TypeScript** - Tipado estÃ¡tico
- **Ollama API** - Backend de IA local
- **Llama3** - Modelo de lenguaje

## ğŸ“± Plataformas Soportadas

- âœ… iOS 11+
- âœ… Android 7.0+ (API 24)

## ğŸ” Estructura del Proyecto

```
src/
â”œâ”€â”€ App.tsx              # Componente principal
â”œâ”€â”€ types/
â”‚   â””â”€â”€ Message.ts       # Tipos TypeScript
â””â”€â”€ styles/
    â””â”€â”€ styles.ts        # Estilos globales
```

## ğŸ› SoluciÃ³n de Problemas

### Error de conexiÃ³n

```
Error: Network request failed
```

**SoluciÃ³n:** Verifica que Ollama estÃ© ejecutÃ¡ndose:

```bash
curl http://localhost:11434/api/tags
```

### El modelo no responde

```
Error del servidor: 404
```

**SoluciÃ³n:** Instala el modelo:

```bash
ollama pull llama3
```

### iOS no conecta a localhost

En iOS Simulator, usa la IP de tu Mac:

```typescript
const response = await fetch('http://192.168.1.XXX:11434/api/chat', {
```

## ğŸ“„ Licencia

MIT License - ve el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

<div align="center">

**Â¿Te gusta el proyecto?** â­ Dale una estrella

[Reportar Bug](https://github.com/tu-usuario/chat-app-llama3/issues) â€¢ 
[Solicitar Feature](https://github.com/tu-usuario/chat-app-llama3/issues) â€¢ 
[Contribuir](CONTRIBUTING.md)

</div>